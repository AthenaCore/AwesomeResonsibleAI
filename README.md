[![Awesome](awesome.svg)](https://github.com/AthenaCore/AwesomeResponsibleAI)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-YES-green.svg)](https://github.com/AthenaCore/AwesomeResponsibleAI/graphs/commit-activity)
![GitHub](https://img.shields.io/badge/Release-PROD-yellow.svg)
![GitHub](https://img.shields.io/badge/Languages-MULTI-blue.svg)
![GitHub](https://img.shields.io/badge/License-MIT-lightgrey.svg)
[![GitHub](https://img.shields.io/twitter/follow/athenacoreai.svg?label=Follow)](https://twitter.com/athenacoreai)

# Awesome Responsible AI
A curated list of awesome academic research, books, code of ethics, newsletters, principles, podcast, reports, tools, regulations and standards related to Responsible AI and Human-Centered AI.

## Contents

- [Academic Research](#academic-research)
- [Books](#books)
- [Code of Ethics](#code-of-ethics)
- [Data Sets](#data-sets)
- [Institutes](#institutes)
- [Newsletters](#newsletters)
- [Principles](#principles)
- [Podcasts](#podcasts)
- [Reports](#reports)
- [Tools](#tools)
- [Regulations](#regulations)
- [Standards](#standards)

## Academic Research

### Bias

- Towards a Standard for Identifying and Managing Bias in Artificial Intelligence ([Schwartz, Reva et al., 2022]()) `NIST`

### Challenges

- Underspecification presents challenges for credibility in modern machine learning. ([D'AMOUR, Alexander, et al., 2020](https://arxiv.org/abs/2011.03395)) `Google`

### Drift

- FreaAI: Automated extraction of data slices to test machine learning models ([Ackerman, S. et al. 2021](https://arxiv.org/pdf/2108.05620.pdf)) `IBM`
- Machine Learning Model Drift Detection Via Weak Data Slices ([Ackerman, S. et al. 2021](https://arxiv.org/pdf/2108.05319.pdf)) `IBM`

### Explainability

- Efficient Data Representation by Selecting Prototypes with Importance Weights ([Gurumoorthy et al., 2019](https://arxiv.org/abs/1707.01212)) `Amazon Development Center` `IBM Research`
- Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives)) `University of Michigan` `IBM Research`
- Contrastive Explanations Method with Monotonic Attribute Functions ([Luss et al., 2019](https://arxiv.org/abs/1905.12698))
- "Why Should I Trust You?": Explaining the Predictions of Any Classifier (LIME) ([Ribeiro et al. 2016](https://arxiv.org/abs/1602.04938),  [Github](https://github.com/marcotcr/lime)) `University of Washington`
- A Unified Approach to Interpreting Model Predictions (SHAP) ([Lundberg, et al. 2017](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions),  [Github](https://github.com/slundberg/shap)) `University of Washington`
- Teaching AI to Explain its Decisions ([Hind et al., 2019](https://doi.org/10.1145/3306618.3314273)) `IBM Research`
- Boolean Decision Rules via Column Generation (Light Edition) ([Dash et al., 2018](https://papers.nips.cc/paper/7716-boolean-decision-rules-via-column-generation)) `IBM Research`
- Generalized Linear Rule Models ([Wei et al., 2019](http://proceedings.mlr.press/v97/wei19a.html)) `IBM Research`
- Improving Simple Models with Confidence Profiles ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/8231-improving-simple-models-with-confidence-profiles)) `IBM Research`
- Towards Robust Interpretability with Self-Explaining Neural Networks ([Alvarez-Melis et al., 2018](https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks)) `MIT`
- Leveraging Latent Features for Local Explanations ([Luss et al., 2019](https://arxiv.org/abs/1905.12698)) `IBM Research` `University of Michigan`

### Fairness

- [Caton, S., & Haas, C. (2020). Fairness in machine learning: A survey. ACM Computing Surveys](https://dl.acm.org/doi/10.1145/3616865)
- [LiFT: A Scalable Framework for Measuring Fairness in ML Applications](https://engineering.linkedin.com/blog/2020/lift-addressing-bias-in-large-scale-ai-applications) ([Vasuvedan et al., 2020](https://arxiv.org/abs/2008.07433)) `LinkedIn`

### Ethical Data Products

- [Building Inclusive Products Through A/B Testing](https://engineering.linkedin.com/blog/2020/building-inclusive-products-through-a-b-testing) ([Saint-Jacques et al, 2020](https://arxiv.org/pdf/2002.05819)) `LinkedIn`

### Sustainability

- Energy and policy considerations for deep learning in NLP ([Strubell, E. et al. 2019](https://arxiv.org/abs/1906.02243))
- Quantifying the carbon emissions of machine learning. ([Lacoste, A. et al. 2019](https://arxiv.org/abs/1910.09700))
- Carbon emissions and large neural network training. ([Patterson, D. et al. 2021](https://arxiv.org/abs/2104.10350)) 
- The Energy and Carbon Footprint of Training End-to-End Speech Recognizers. ([Parcollet, T., & Ravanelli, M. 2021](https://hal.archives-ouvertes.fr/hal-03190119/document))
- Sustainable AI: AI for sustainability and the sustainability of AI ([van Wynsberghe, A. 2021](https://link.springer.com/article/10.1007/s43681-021-00043-6)). AI and Ethics, 1-6
- Green Algorithms: Quantifying the carbon emissions of computation ([Lannelongue, L. et al. 2020](https://arxiv.org/abs/2007.07610))
- Machine Learning: The High Interest Credit Card of Technical Debt ([Sculley, D. et al. 2014](https://research.google/pubs/pub43146/)) `Google`

### Collections

- Google Research on Responsible AI: https://research.google/pubs/?collection=responsible-ai `Google`
- Pipeline-Aware Fairness: http://fairpipe.dssg.io

### Reproducible/Non-Reproducible Research

- [Papers with Code](https://paperswithcode.com)
- [Papers without Code](https://www.paperswithoutcode.com)

## Books

### Open Access

- Interpretable Machine Learning ([Molnar, C., 2021](https://christophm.github.io/interpretable-ml-book/)) `Explainability` `Interpretability` `Transparency` `R`
- Explanatory Model Analysis ([Biecek et al., 2020](https://ema.drwhy.ai)) `Explainability` `Interpretability` `Transparency` `R`

### Commercial / Propietary / Closed Access

- Trust in Machine Learning ([Varshney, K., 2022](https://www.manning.com/books/trust-in-machine-learning)) `Safety` `Privacy` `Drift` `Fairness` `Interpretability` `Explainability`
- Interpretable AI ([Thampi, A., 2022](https://www.manning.com/books/interpretable-ai)) `Explainability` `Fairness` `Interpretability` 
- AI Fairness ([Mahoney, T., Varshney, K.R., Hind, M., 2020](https://learning.oreilly.com/library/view/ai-fairness/9781492077664/) `Report` `Fairness`
- Practical Fairness ([Nielsen, A., 2021](https://learning.oreilly.com/library/view/practical-fairness/9781492075721/)) `Fairness`
- Hands-On Explainable AI (XAI) with Python ([Rothman, D., 2020](https://www.packtpub.com/product/hands-on-explainable-ai-xai-with-python/9781800208131)) `Explainability`
- AI and the Law ([Kilroy, K., 2021](https://learning.oreilly.com/library/view/ai-and-the/9781492091837/)) `Report` `Trust` `Law`
- Responsible Machine Learning ([Hall, P., Gill, N., Cox, B., 2020](https://learning.oreilly.com/library/view/responsible-machine-learning/9781492090878/)) `Report` `Law`  `Compliance` `Safety` `Privacy` 
- [Privacy-Preserving Machine Learning](https://www.manning.com/books/privacy-preserving-machine-learning)
- [Human-In-The-Loop Machine Learning: Active Learning and Annotation for Human-Centered AI](https://www.manning.com/books/human-in-the-loop-machine-learning)
- [Interpretable Machine Learning With Python: Learn to Build Interpretable High-Performance Models With Hands-On Real-World Examples](https://www.packtpub.com/product/interpretable-machine-learning-with-python/9781800203907)
- Responsible AI ([Hall, P., Chowdhury, R., 2023](https://learning.oreilly.com/library/view/responsible-ai/9781098102425/)) `Governance` `Safety` `Drift`

## Code of Ethics

- [ACS Code of Professional Conduct](https://www.acs.org.au/content/dam/acs/rules-and-regulations/Code-of-Professional-Conduct_v2.1.pdf) by Australian ICT (Information and Communication Technology)
- [AI Standards Hub](https://aistandardshub.org)
- [Association for Computer Machinery's Code of Ethics and Professional Conduct](https://www.acm.org/code-of-ethics)
- [IEEE Global Initiative for Ethical Considerations in Artificial Intelligence (AI) and Autonomous Systems (AS)](https://ethicsinaction.ieee.org/)
- [ISO/IEC's Standards for Artificial Intelligence](https://www.iso.org/committee/6794475/x/catalogue/)

## Data Sets

- [An ImageNet replacement for self-supervised pretraining without humans](https://www.robots.ox.ac.uk/~vgg/research/pass/)
- [Huggingface Data Sets](https://huggingface.co/datasets)

## Institutes

- [Ada Lovelace Institute](https://www.adalovelaceinstitute.org/) `United Kingdom`
- [European Centre for Algorithmic Transparency](https://algorithmic-transparency.ec.europa.eu/index_en)
- [Center for Responsible AI](https://airesponsibly.com/)
- [Montreal AI Ethics Institute](https://montrealethics.ai/)
- [Munich Center for Technology in Society (IEAI)](https://ieai.mcts.tum.de/) `Germany`
- [National AI Centre's Responsible AI Network](https://www.csiro.au/RAIN) `Australia` 
- [Open Data Institute](https://theodi.org/) `United Kingdom`
- [Stanford University Human-Centered Artificial Intelligence (HAI)](https://hai.stanford.edu) `United States of America`
- [The Institute for Ethical AI & Machine Learning](https://ethical.institute/)
- [University of Oxford Institute for Ethics in AI](https://www.oxford-aiethics.ox.ac.uk/) `United Kingdom`

## Newsletters

- [Import AI](https://jack-clark.net)
- [The AI Ethics Brief](https://brief.montrealethics.ai)
- [The Machine Learning Engineer](https://ethical.institute/mle.html) 

## Principles

- [Allianz's Principles for a responsible usage of AI ](https://www.allianz.com/en/about-us/strategy-values/data-ethics-and-responsible-ai.html) `Allianz`
- [Asilomar AI principles](https://futureoflife.org/open-letter/ai-principles/)
- [European Commission's Guidelines for Trustworthy AI](https://ec.europa.eu/futurium/en/ai-alliance-consultation)
- [Google's AI Principles](https://ai.google/principles/) `Google`
- [IEEE's Ethically Aligned Design](https://ethicsinaction.ieee.org/) `IEEE`
- [Microsoft's AI principles](https://www.microsoft.com/en-us/ai/responsible-ai) `Microsoft`
- [OECD's AI principles](https://oecd.ai/en/ai-principles) `OECD`
- [Telefonica's AI principles](https://www.telefonica.com/en/sustainability-innovation/how-we-work/business-principles/#artificial-intelligence-principles) `Telefonica`
- [The Institute for Ethical AI & Machine Learning: The Responsible Machine Learning Principles](https://ethical.institute/principles.html)

Additional:

- [FAIR Principles](https://www.go-fair.org/fair-principles/) `Findability` `Accessibility` `Interoperability` `Reuse`

## Podcasts

- [The Human-Centered AI Podcast](https://podcasts.apple.com/us/podcast/the-human-centered-ai-podcast/id1499839858)
- [Responsible AI Podcast](https://open.spotify.com/show/63Fx70r96P3ghWavisvPEQ)
- [Trustworthy AI](https://marketing.truera.com/trustworthy-ai-podcast)

## Reports

- [AI Incident Database](https://incidentdatabase.ai)
- [Inferring Concept Drift Without Labeled Data, 2021](https://concept-drift.fastforwardlabs.com) `Drift`
- [Interpretability, Fast Forward Labs, 2020](https://ff06-2020.fastforwardlabs.com) `Interpretability`
- [ML Commons Safety Benchmark for general purpose AI chat model](https://mlcommons.org/benchmarks/ai-safety/general_purpose_ai_chat_benchmark/)
- [State of AI](https://www.stateof.ai) - from 2018 up to now -

## Tools

### Bias

- [balance](https://import-balance.org) `Python` `Facebook`

### Causal Inference

- [CausalAI](https://github.com/salesforce/causalai) `Python` `Salesforce`
- [CausalNex](https://causalnex.readthedocs.io) `Python`
- [CausalImpact](https://cran.r-project.org/web/packages/CausalImpact) `R`
- [Causalinference](https://causalinferenceinpython.org) `Python`
- [CIMTx: Causal Inference for Multiple Treatments with a Binary Outcome](https://cran.r-project.org/web/packages/CIMTx) `R`
- [dagitty](https://cran.r-project.org/web/packages/dagitty) `R`
- [DoWhy](https://github.com/Microsoft/dowhy) `Python` `Microsoft`
- [mediation: Causal Mediation Analysis](https://cran.r-project.org/web/packages/mediation) `R`
- [MRPC](https://cran.r-project.org/web/packages/MRPC) `R`

### Differential Privacy

- [BackPACK](https://toiaydcdyywlhzvlob.github.io/backpack) `Python`
- [DataSynthesizer: Privacy-Preserving Synthetic Datasets](https://github.com/DataResponsibly/DataSynthesizer) `Python` `Drexel University` `University of Washington`
- [diffpriv](https://github.com/brubinstein/diffpriv) `R`
- [Diffprivlib](https://github.com/IBM/differential-privacy-library) `Python` `IBM`
- [Discrete Gaussian for Differential Privacy](https://github.com/IBM/discrete-gaussian-differential-privacy) `Python` `IBM`
- [Opacus](https://opacus.ai) `Python` `Facebook`
- [PyVacy: Privacy Algorithms for PyTorch](https://github.com/ChrisWaites/pyvacy) `Python`
- [SEAL](https://github.com/Microsoft/SEAL) `Python` `Microsoft`
- [SmartNoise](https://github.com/opendp/smartnoise-core) `Python` `OpenDP`
- [Tensorflow Privacy](https://github.com/tensorflow/privacy) `Python` `Google`

### Drift

- [Alibi Detect](https://github.com/SeldonIO/alibi-detect) `Python`
- [Deepchecks](https://github.com/deepchecks/deepchecks) `Python`
- [drifter](https://cran.r-project.org/web/packages/drifter/) `R`
- [Evidently](https://github.com/evidentlyai/evidently) `Python`
- [nannyML](https://github.com/NannyML/nannyml) `Python`
- [phoenix](https://github.com/Arize-ai/phoenix) `Python`

### Fairness

- [Aequitas' Bias & Fairness Audit Toolkit](http://aequitas.dssg.io/) `Python`
- [AI360 Toolkit](https://github.com/Trusted-AI/AIF360) `Python` `R` `IBM`
- [EDFfair: Explicitly Deweighted Features](https://github.com/matloff/EDFfair) `R`
- [Fairlearn](https://fairlearn.org) `Python` `Microsoft`
- [Fairmodels](https://fairmodels.drwhy.ai) `R` `University of California`
- [fairness](https://cran.r-project.org/web/packages/fairness/) `R`
- [FairRankTune](https://kcachel.github.io/fairranktune/) `Python`
- [FairPAN - Fair Predictive Adversarial Network](https://modeloriented.github.io/FairPAN/) `R`
- [Themis ML](https://github.com/cosmicBboy/themis-ml) `Python`
- [What-If Tool](https://github.com/PAIR-code/what-if-tool) `Python` `Google`

### Frameworks

- [Mercury](https://www.bbvaaifactory.com/mercury/) `Python` `BBVA`
- [Deepchecks](https://github.com/deepchecks/deepchecks) `Python`

### Interpretability/Explicability

- [AI360 Toolkit](https://github.com/Trusted-AI/AIF360) `Python` `R` `IBM`
- [aorsf: Accelerated Oblique Random Survival Forests](https://cran.r-project.org/web/packages/aorsf/index.html) `R`
- [breakDown: Model Agnostic Explainers for Individual Predictions](https://cran.r-project.org/web/packages/breakDown/index.html) `R`
- [captum](https://github.com/pytorch/captum) `Python` `PyTorch`
- [ceterisParibus: Ceteris Paribus Profiles](https://cran.r-project.org/web/packages/ceterisParibus/index.html) `R`
- [DALEX: moDel Agnostic Language for Exploration and eXplanation](https://dalex.drwhy.ai) `Python` `R`
- [DALEXtra: extension for DALEX](https://modeloriented.github.io/DALEXtra) `Python` `R`
- [ecco](https://pypi.org/project/ecco/) [article](https://jalammar.github.io/explaining-transformers/) `Python`
- [eli5](https://github.com/TeamHG-Memex/eli5) `Python`
- [eXplainability Toolbox](https://ethical.institute/xai.html) `Python`
- [ExplainerHub](https://explainerdashboard.readthedocs.io/en/latest/index.html) [in github](https://github.com/oegedijk/explainerdashboard) `Python` 
- [fasttreeshap](https://github.com/linkedin/fasttreeshap) `Python` `LinkedIn`
- [FAT Forensics](https://fat-forensics.org/) `Python`
- [flashlight](https://github.com/mayer79/flashlight) `R`
- [Human Learn](https://github.com/koaning/human-learn) `Python`
- [hstats](https://cran.r-project.org/web/packages/hstats/index.html) `R`
- [innvestigate](https://github.com/albermax/innvestigate) `Python` `Neural Networks`
- [intepretML](https://interpret.ml) `Python`
- [interactions: Comprehensive, User-Friendly Toolkit for Probing Interactions](https://cran.r-project.org/web/packages/interactions/index.html) `R`
- [kernelshap: Kernel SHAP](https://cran.r-project.org/web/packages/kernelshap/index.html) `R`
- [lime: Local Interpretable Model-Agnostic Explanations](https://cran.r-project.org/web/packages/lime/index.html) `R`
- [Network Dissection](http://netdissect.csail.mit.edu) `Python` `Neural Networks` `MIT` 
- [Shap](https://github.com/slundberg/shap) `Python`
- [Shapash](https://github.com/maif/shapash) `Python`
- [shapviz](https://cran.r-project.org/web/packages/shapviz/index.html) `R`
- [Skater](https://github.com/oracle/Skater) `Python` `Oracle`
- [survex](https://github.com/ModelOriented/survex) `R`
- [TCAV (Testing with Concept Activation Vectors)](https://pypi.org/project/tcav/) `Python` 
- [truelens](https://pypi.org/project/trulens/) `Python` `Truera`
- [truelens-eval](https://pypi.org/project/trulens-eval/) `Python` `Truera`
- [pre: Prediction Rule Ensembles](https://cran.r-project.org/web/packages/pre/index.html) `R`
- [Vetiver](https://rstudio.github.io/vetiver-r/) `R` `Python` `Posit`
- [vivid](https://cloud.r-project.org/web/packages/vivid/index.html) `R`
- [XAI - An eXplainability toolbox for machine learning](https://github.com/EthicalML/xai) `Python` `The Institute for Ethical Machine Learning`
- [Zennit](https://github.com/chr5tphr/zennit) `Python`

### Interpretable Models

- [imodels](https://github.com/csinva/imodels) `Python`
- [imodelsX](https://github.com/csinva/imodelsX) `Python`

### LLM Evaluation

- [Inspect](https://ukgovernmentbeis.github.io/inspect_ai/) `AISI` `Python`
- [Prometheus](https://github.com/prometheus-eval/prometheus) `Python`

### Performance (& Automated ML)

- [auditor](https://github.com/ModelOriented/auditor) `R`
- [automl: Deep Learning with Metaheuristic](https://cran.r-project.org/web/packages/automl/index.html) `R`
- [AutoKeras](https://github.com/keras-team/autokeras) `Python`
- [Auto-Sklearn](https://github.com/automl/auto-sklearn) `Python`
- [DataPerf](https://sites.google.com/mlcommons.org/dataperf/) `Python` `Google`
- [deepchecks](https://deepchecks.com) `Python`
- [EloML](https://github.com/ModelOriented/EloML) `R`
- [Featuretools](https://www.featuretools.com) `Python`
- [LOFO Importance](https://github.com/aerdem4/lofo-importance) `Python`
- [forester](https://modeloriented.github.io/forester/) `R`
- [metrica: Prediction performance metrics](https://adriancorrendo.github.io/metrica/) `R`
- [NNI: Neural Network Intelligence](https://github.com/microsoft/nni) `Python` `Microsoft`
- [performance](https://github.com/easystats/performance) `R`
- [TensorFlow Model Remediation ](https://github.com/tensorflow/model-remediation) `Python` `Google`
- [TPOT](http://epistasislab.github.io/tpot/) `Python`
- [Unleash](https://www.getunleash.io) `Python`
- [Yellowbrick](https://www.scikit-yb.org/en/latest/) `Python`
- [WeightWatcher](https://github.com/CalculatedContent/WeightWatcher)  `Python`

### Poisoning

- [Nightshade](https://nightshade.cs.uchicago.edu) `University of Chicago`
- [Glaze](https://glaze.cs.uchicago.edu) `University of Chicago`

### Responsible AI toolkit

- [Dr. Why](https://github.com/ModelOriented/DrWhy) `R` `Warsaw University of Technology`
- [Responsible AI Widgets](https://github.com/microsoft/responsible-ai-widgets) `R` `Microsoft`
- [The Data Cards Playbook](https://pair-code.github.io/datacardsplaybook/) `Python` `Google`

### Security

- [Modelscan](https://github.com/protectai/modelscan) `Python`
- [NB Defense](https://nbdefense.ai) `Python`
- [Rebuff Playground](https://www.rebuff.ai/playground) `Python`

### Sustainability

- [Code Carbon](https://github.com/mlco2/codecarbon) `Python`
- [Azure Sustainability Calculator](https://appsource.microsoft.com/en-us/product/power-bi/coi-sustainability.sustainability_dashboard) `Microsoft`
- [Computer Progress](https://www.computerprogress.com)

## Regulations

- [Data Protection Laws of the Word](https://www.dlapiperdataprotection.com)
- [SCL Artificial Intelligence Contractual Clauses](https://www.scl.org/wp-content/uploads/2024/02/AI-Clauses-Project-October-2023-final-1.pdf)

### European Union

- [General Data Protection Regulation GDPR](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A32016R0679) - Legal text for the EU GDPR regulation 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC
- [GDPR.EU Guide](https://gdpr.eu/) - A project co-funded by the Horizon 2020 Framework programme of the EU which provides a resource for organisations and individuals researching GDPR, including a library of straightforward and up-to-date information to help organisations achieve GDPR compliance ([Legal Text](https://www.govinfo.gov/content/pkg/USCODE-2012-title5/pdf/USCODE-2012-title5-partI-chap5-subchapII-sec552a.pdf)).
- [AI ACT](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206)

### United States

- State consumer privacy laws: California ([CCPA](https://www.oag.ca.gov/privacy/ccpa) and its amendment, [CPRA](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202120220AB1490)), Virginia ([VCDPA](https://lis.virginia.gov/cgi-bin/legp604.exe?212+sum+HB2307)), and Colorado ([ColoPA](https://leg.colorado.gov/sites/default/files/documents/2021A/bills/2021a_190_rer.pdf)).
- Specific and limited privacy data laws: [HIPAA](https://www.cdc.gov/phlp/publications/topic/hipaa.html), [FCRA](https://www.ftc.gov/enforcement/statutes/fair-credit-reporting-act), [FERPA](https://www.cdc.gov/phlp/publications/topic/ferpa.html), [GLBA](https://www.ftc.gov/tips-advice/business-center/privacy-and-security/gramm-leach-bliley-act), [ECPA](https://bja.ojp.gov/program/it/privacy-civil-liberties/authorities/statutes/1285), [COPPA](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule), [VPPA](https://www.law.cornell.edu/uscode/text/18/2710) and [FTC](https://www.ftc.gov/enforcement/statutes/federal-trade-commission-act).
- [EU-U.S. and Swiss-U.S. Privacy Shield Frameworks](https://www.privacyshield.gov/welcome) - The EU-U.S. and Swiss-U.S. Privacy Shield Frameworks were designed by the U.S. Department of Commerce and the European Commission and Swiss Administration to provide companies on both sides of the Atlantic with a mechanism to comply with data protection requirements when transferring personal data from the European Union and Switzerland to the United States in support of transatlantic commerce.
- [Executive Order on Maintaining American Leadership in AI](https://www.whitehouse.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence/) - Official mandate by the President of the US to 
[Privacy Act of 1974](https://www.justice.gov/opcl/privacy-act-1974) - The privacy act of 1974 which establishes a code of fair information practices that governs the collection, maintenance, use and dissemination of information about individuals that is maintained in systems of records by federal agencies.
- [Privacy Protection Act of 1980](https://epic.org/privacy/ppa/) - The Privacy Protection Act of 1980 protects journalists from being required to turn over to law enforcement any work product and documentary materials, including sources, before it is disseminated to the public.
- [AI Bill of Rights](https://www.whitehouse.gov/ostp/ai-bill-of-rights/) - The Blueprint for an AI Bill of Rights is a guide for a society that protects all people from IA threats based on five principles: Safe and Effective Systems, Algorithmic Discrimination Protections, Data Privacy, Notice and Explanation, and  Human Alternatives, Consideration, and Fallback.

## Standards

### Definition

**What are standards?**

Standards are **voluntary**, **consensus soluctions**. They document an **agreement** on how a material, product, process, or serice should be **specified**, **performed** or **delivered**. They keep people safe and **ensure things work**. They create **confidence** and provide **security** for investment.

### ISO/IEC Standards

Domain | Standard | Status | URL
---|---|---|---
AI Concepts and Terminology| ISO/IEC 22989:2022 Information technology — Artificial intelligence — Artificial intelligence concepts and terminology | Published | https://www.iso.org/standard/74296.html
AI Risk Management | ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk management | Published | 	https://www.iso.org/standard/77304.html
AI Management System | ISO/IEC DIS 42001 Information technology — Artificial intelligence — Management system | Published | https://www.iso.org/standard/81230.html
Trustworthy AI | ISO/IEC TR 24028:2020 Information technology — Artificial intelligence — Overview of trustworthiness in artificial intelligence | Published | https://www.iso.org/standard/77608.html
Biases in AI | ISO/IEC TR 24027:2021 Information technology — Artificial intelligence (AI) — Bias in AI systems and AI aided decision making | Published | https://www.iso.org/standard/77607.html
AI Performance | ISO/IEC TS 4213:2022 Information technology — Artificial intelligence — Assessment of machine learning classification performance | Published | https://www.iso.org/standard/79799.html
Ethical and societal concerns | ISO/IEC TR 24368:2022 Information technology — Artificial intelligence — Overview of ethical and societal concerns | Published | https://www.iso.org/standard/78507.html
Explainability | ISO/IEC AWI TS 6254 Information technology — Artificial intelligence — Objectives and approaches for explainability of ML models and AI systems | Under Development | https://www.iso.org/standard/82148.html
AI Sustainability | ISO/IEC AWI TR 20226 Information technology — Artificial intelligence — Environmental sustainability aspects of AI systems | Under Development | https://www.iso.org/standard/86177.html
AI Verification and Validation | ISO/IEC AWI TS 17847 Information technology — Artificial intelligence — Verification and validation analysis of AI systems | Under Development | https://www.iso.org/standard/85072.html
AI Controllabitlity | ISO/IEC CD TS 8200 Information technology — Artificial intelligence — Controllability of automated artificial intelligence systems | Published | https://www.iso.org/standard/83012.html
Biases in AI | ISO/IEC CD TS 12791 Information technology — Artificial intelligence — Treatment of unwanted bias in classification and regression machine learning tasks | Under Publication | https://www.iso.org/standard/84110.html
AI Impact Assessment | ISO/IEC AWI 42005 Information technology — Artificial intelligence — AI system impact assessment | Under Development | https://www.iso.org/standard/44545.html
Data Quality for AI/ML | ISO/IEC DIS 5259 Artificial intelligence — Data quality for analytics and machine learning (ML) (1 to 6) | Under Development | https://www.iso.org/standard/81088.html
Data Lifecycle | ISO/IEC FDIS 8183 Information technology — Artificial intelligence — Data life cycle framework | Published | https://www.iso.org/standard/83002.html
Audit and Certification | ISO/IEC CD 42006 Information technology — Artificial intelligence — Requirements for bodies providing audit and certification of artificial intelligence management systems | Under Development | https://www.iso.org/standard/44546.html
Transparency | ISO/IEC AWI 12792 Information technology — Artificial intelligence — Transparency taxonomy of AI systems | Under Development | https://www.iso.org/standard/84111.html
AI Quality | ISO/IEC AWI TR 42106 Information technology — Artificial intelligence — Overview of differentiated benchmarking of AI system quality characteristics | Under Development | https://www.iso.org/standard/86903.html
Synthetic Data | ISO/IEC AWI TR 42103 Information technology — Artificial intelligence — Overview of synthetic data in the context of AI systems | Under Development | https://www.iso.org/standard/86899.html
AI Security | ISO/IEC AWI 27090 Cybersecurity — Artificial Intelligence — Guidance for addressing security threats and failures in artificial intelligence systems | Under Development | https://www.iso.org/standard/56581.html
AI Privacy | ISO/IEC AWI 27091 Cybersecurity and Privacy — Artificial Intelligence — Privacy protection | Under Development	 | https://www.iso.org/standard/56582.html
AI Governance | ISO/IEC 38507:2022 Information technology — Governance of IT — Governance implications of the use of artificial intelligence by organizations | Published | https://www.iso.org/standard/56641.html
AI Safety | ISO/IEC CD TR 5469 Artificial intelligence — Functional safety and AI systems | Under Development | https://www.iso.org/standard/81283.html
Beneficial AI Systems | ISO/IEC AWI TR 21221 Information technology – Artificial intelligence – Beneficial AI systems | Under Development  | https://www.iso.org/standard/86690.html

### NIST Standards

- [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework)


Additional standards can be found using the [Standards Database](https://aistandardshub.org/ai-standards-search/).
