[![Awesome](awesome.svg)](https://github.com/AthenaCore/AwesomeResponsibleAI)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-YES-green.svg)](https://github.com/AthenaCore/AwesomeResponsibleAI/graphs/commit-activity)
![GitHub](https://img.shields.io/badge/Release-PROD-yellow.svg)
![GitHub](https://img.shields.io/badge/Languages-MULTI-blue.svg)
![GitHub](https://img.shields.io/badge/License-MIT-lightgrey.svg)
[![GitHub](https://img.shields.io/twitter/follow/athenacoreai.svg?label=Follow)](https://twitter.com/athenacoreai)

# Awesome Responsible AI
A curated list of awesome academic research, books, code of ethics, newsletters, principles, podcast, tools and regulations related to Responsible AI.

## Contents

- [Academic Research](#academic-research)
- [Books](#books)
- [Code of Ethics](#code-of-ethics)
- [Newsletters](#newsletters)
- [Principles](#principles)
- [Podcasts](#podcasts)
- [Tools](#tools)
- [Regulations](#regulations)

## Academic Research

### Data explanation

- ProtoDash ([Gurumoorthy et al., 2019](https://arxiv.org/abs/1707.01212))
- Disentangled Inferred Prior VAE ([Kumar et al., 2018](https://openreview.net/forum?id=H1kG7GZAW))

### Local post-hoc explanation 

- ProtoDash ([Gurumoorthy et al., 2019](https://arxiv.org/abs/1707.01212))
- Contrastive Explanations Method ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives))
- Contrastive Explanations Method with Monotonic Attribute Functions ([Luss et al., 2019](https://arxiv.org/abs/1905.12698))
- LIME ([Ribeiro et al. 2016](https://arxiv.org/abs/1602.04938),  [Github](https://github.com/marcotcr/lime))
- SHAP ([Lundberg, et al. 2017](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions),  [Github](https://github.com/slundberg/shap))

### Local direct explanation

- Teaching AI to Explain its Decisions ([Hind et al., 2019](https://doi.org/10.1145/3306618.3314273)) 
   
### Global direct explanation

- Boolean Decision Rules via Column Generation (Light Edition) ([Dash et al., 2018](https://papers.nips.cc/paper/7716-boolean-decision-rules-via-column-generation))
- Generalized Linear Rule Models ([Wei et al., 2019](http://proceedings.mlr.press/v97/wei19a.html))

### Global post-hoc explanationÂ 

- ProfWeight ([Dhurandhar et al., 2018](https://papers.nips.cc/paper/8231-improving-simple-models-with-confidence-profiles))

### Explainability and fairness metrics
- Towards Robust Interpretability with Self-Explaining Neural Networks ([Alvarez-Melis and Jaakkola, 2018](https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks)) `MIT`
- Leveraging Latent Features for Local Explanations ([Luss et al., 2019](https://arxiv.org/abs/1905.12698)) `IBM Research` `University of Michigan`
- [LiFT: A Scalable Framework for Measuring Fairness in ML Applications](https://engineering.linkedin.com/blog/2020/lift-addressing-bias-in-large-scale-ai-applications) ([Vasuvedan et al., 2020](https://arxiv.org/abs/2008.07433)) `LinkedIn`

### Ethical Data Products

- [Building Inclusive Products Through A/B Testing](https://engineering.linkedin.com/blog/2020/building-inclusive-products-through-a-b-testing) ([Saint-Jacques et al, 2020](https://arxiv.org/pdf/2002.05819)) `LinkedIn`

## Books

### Open Access

- [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)
- [Explanatory Model Analysis](https://ema.drwhy.ai)
- [Inferring Concept Drift Without Labeled Data](https://concept-drift.fastforwardlabs.com)

### Commercial / Propietary / Closed Access

- [Trust in Machine Learning](https://www.manning.com/books/trust-in-machine-learning)
- [Interpretable AI](https://www.manning.com/books/interpretable-ai)
- [AI Fairness](https://learning.oreilly.com/library/view/ai-fairness/9781492077664/)
- [Practical Fairness](https://learning.oreilly.com/library/view/practical-fairness/9781492075721/)
- [Hands-On Explainable AI (XAI) with Python](https://www.packtpub.com/product/hands-on-explainable-ai-xai-with-python/9781800208131)
- [AI and the Law](https://learning.oreilly.com/library/view/ai-and-the/9781492091837/)
- [Responsible Machine Learning](https://learning.oreilly.com/library/view/responsible-machine-learning/9781492090878/)
- [Privacy-Preserving Machine Learning](https://www.manning.com/books/privacy-preserving-machine-learning)
- [Human-In-The-Loop Machine Learning: Active Learning and Annotation for Human-Centered AI](https://www.manning.com/books/human-in-the-loop-machine-learning)
- [Interpretable Machine Learning With Python: Learn to Build Interpretable High-Performance Models With Hands-On Real-World Examples](https://www.packtpub.com/product/interpretable-machine-learning-with-python/9781800203907)
- [Responsible AI](https://learning.oreilly.com/library/view/responsible-ai/9781098102425/)

## Code of Ethics
## Newsletters

- [Import AI](https://jack-clark.net)
- [The AI Ethics Brief](https://brief.montrealethics.ai)

## Principles
## Podcasts

## Tools

### Fairness

- [Aequitas' Bias & Fairness Audit Toolkit](http://aequitas.dssg.io/) `Python`

### Interpretability/Explicability

- [Dalex](https://dalex.drwhy.ai) `Python` `R`
- [eXplainability Toolbox](https://ethical.institute/xai.html) `Python`
- [Fairlearn](https://fairlearn.org) `Python`
- [IBM's AI Explainability 360 Open Source Toolkit](http://aix360.mybluemix.net/) `Python`

### Drift

- [Evidently](https://github.com/evidentlyai/evidently) `Python`

### Non-active

- [FAT Forensics](https://fat-forensics.org/) (last update 19 May 2020) `Python`

## Regulations
